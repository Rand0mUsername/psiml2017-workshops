{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow\n",
    "\n",
    "\n",
    "#### What is it?\n",
    "* System for workign with computational graphs over Tensor objects (analogous to numpy ndarrays).\n",
    "* Automatic differentiation (backpropogation) for Variable objects.\n",
    "\n",
    "#### Why?\n",
    "* Naive implementation of basic layers is easy, but opitimized implementation is harder.\n",
    "* GPU implementation is even harder.\n",
    "* In academia, industry, and PSIML projects, you are far more likely to use a framework that to code ConvNets from scratch.\n",
    "\n",
    "#### How to learn it?\n",
    "* This notebook will try to cover a wide range of stuff.\n",
    "* References to additional resources will be given where needed.\n",
    "* You're welcome to use many excellent tutorials on the web, including the [one from Google](https://www.tensorflow.org/get_started/get_started).\n",
    "\n",
    "First import TensorFlow module, as we will be using it throughout. This also verifies that TensorFlow installation is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from utils.vis_utils import visualize_grid\n",
    "from utils.data_utils import load_CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "We will use CIFAR-10 dataset from https://www.cs.toronto.edu/~kriz/cifar.html. The dataset contains 50,000 training images and 10,000 test images. All images are 32 x 32 x 3. Each image is classified as one of 10 classes.\n",
    "\n",
    "We call a utility function to load CIFAR-10 data.\n",
    "Then we divide the data into training, validation, and test set.\n",
    "Finally we normalize data by subtracting mean image from each sample.\n",
    "Note that the mean image is computed from training set only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_CIFAR10_data(num_training = 49000, num_validation = 1000, num_test = 10000):\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, mean_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke the above function to get data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test, mean_image = get_CIFAR10_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the data has expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_CIFAR10_sample(X, y, sample_count = 10, class_count = 10):\n",
    "    set_size = X.shape[0]\n",
    "\n",
    "    # Randomize dataset.\n",
    "    data = np.ndarray([set_size, 2], dtype = np.int32)\n",
    "    data[:, 0] = list(range(set_size))\n",
    "    data[:, 1] = y\n",
    "    data[:, :] = data[np.random.permutation(set_size)]\n",
    "    \n",
    "    # Select samples.\n",
    "    selection = { i : [] for i in range(class_count) }\n",
    "    count = 0\n",
    "    for (ind, cls) in data:\n",
    "        if len(selection[cls]) < sample_count:\n",
    "            selection[cls] += [ind]\n",
    "            count += 1\n",
    "        if count == class_count * sample_count:\n",
    "            break\n",
    "    \n",
    "    # Ensure that we found enough samples.\n",
    "    assert count == class_count * sample_count\n",
    "    \n",
    "    # Flatten list.\n",
    "    selection_flat = [item for cls in range(class_count) for item in selection[cls]]\n",
    "    \n",
    "    # Visualize samples.\n",
    "    plt.figure(figsize = (12, 12))\n",
    "    plt.imshow(visualize_grid((X[selection_flat, :, :, :] + np.reshape(mean_image, [1, 32, 32, 3]))))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_CIFAR10_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what the classes represent, we can create an array of human-readable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining and examining a simple network\n",
    "\n",
    "We define a simple network consisting of basic layers:\n",
    "* convolutional layer,\n",
    "* max-pooling layer,\n",
    "* fully connected layer, and\n",
    "* ReLU activation function.\n",
    "\n",
    "TensorFlow supports many other layer types and activations. See https://www.tensorflow.org/api_guides/python/nn for official API documentation. \n",
    "\n",
    "The following line clears any network that might already exist in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorBoard log file\n",
    "\n",
    "We can use the TensorBoard to visualize our training data. TensorBoard parses log files (also called event files) generated by TensorFlow. We will be placing those files in a separate dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_dir = './logs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new event file is created by instantiating a `tf.FileWriter` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders for data\n",
    "First we define placeholders for input data (input image and its label) using `tf.placeholder`.\n",
    "We will eventually bind these to actual numerical data values.\n",
    "\n",
    "We choose to represent input data as 4D tensors whose shape is N x H x W x C, where:\n",
    "* N is the number of examples in a batch (batch size)\n",
    "* H is the height of each image in pixels\n",
    "* W is the height of each image in pixels\n",
    "* C is the number of channels (usually 3: R, G, B)\n",
    "\n",
    "This is the right way to represent the data for spatial operations like convolution. For fully connected layers, however, all dimensions except batch size will be collapsed into one.\n",
    "\n",
    "In `tf.placeholder`, if a dimension has value `None`, it will be set automatically once actual data is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_input():\n",
    "    X = tf.placeholder(tf.float32, [None, 32, 32, 3], name = 'X')\n",
    "    y = tf.placeholder(tf.int64, [None], name = 'y')\n",
    "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
    "    return X, y, is_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, is_training = setup_input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional and pooling nodes\n",
    "Next we start defining define the main \"body\" of the network.\n",
    "We start by adding a single convolutional layer with bias and ReLU activation.\n",
    "\n",
    "Convolutional layer is created by calling `tf.layers.conv2d`.\n",
    "Returned object is of type `tf.Tensor` and represents output activations of the layer.\n",
    "\n",
    "Bias is enabled by default, so it is not explicitly specified.\n",
    "\n",
    "`padding = 'SAME'` means that we allow padding of roughly half the kernel size (TensorFlow computes this value automatically), to avoid reduction in output size due to boundary effects. The other option is `padding = 'VALID'`, where padding is disabled.\n",
    "\n",
    "We use [tf.layers API](https://www.tensorflow.org/api_docs/python/tf/layers) to generate a whole layer by a single function call. It is also possible to create each parameter and operation node separately, and connect them together, but that quickly becomes cumbersome for bigger networks. See [this tutorial](https://www.tensorflow.org/tutorials/layers) for how to use `tf.layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1 = tf.layers.conv2d(inputs = X, filters = 32, kernel_size = [7, 7], strides = 2, padding = 'SAME', activation=tf.nn.relu, name = 'conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we add a max-pooling node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool1 = tf.layers.max_pooling2d(inputs = conv1, pool_size = [2, 2], strides = 2, padding = 'SAME', name = 'pool1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View default graph in TensorBoard\n",
    "\n",
    "We can write graph data to event file we create above. A graph can be passed to `FileWriter` constructor as well, in which case it is written to file immediately after the file is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to run `tensorboard --logdir=./logs` from console (with your Python environment activated), and see the graph visualized in browser at `http://localhost:6006`.\n",
    "\n",
    "For more details please see official tutorial on [graph visualization](https://www.tensorflow.org/get_started/graph_viz).\n",
    "\n",
    "Note: graph visualization seems to work best in Google Chrome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining parameters and activations\n",
    "Parameter tensors are hidden inside the convolution layer we created, and we don't have a handle on them. To get one, we have to know their names. For that we can either consult TensorBoard visualization, or list all variables in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and pick the one we need by name. The `conv1` prefix in both names refers to the `name` parameter specified when creating the layer, and NOT the Python variable `conv1`.\n",
    "\n",
    "Then we access the tensor itself using `get_tensor_by_name`. For example, we can get the shapes of kernel and bias as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_kernel = tf.get_default_graph().get_tensor_by_name('conv1/kernel:0')\n",
    "conv1_bias = tf.get_default_graph().get_tensor_by_name('conv1/bias:0')\n",
    "print(conv1_kernel.shape)\n",
    "print(conv1_bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes of activation tensors are computed automatically, and can also be accessed using `shape`. Note that these tensors may have unknown dimensions which become known only when acutal input is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conv1.shape)\n",
    "print(pool1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully connected layers\n",
    "Next we append a fully connected layer with 1024 output neurons and ReLU activation.\n",
    "In order to determine the shape of its parameter tensor, we need to know the number of input neurons, which depends on the shape of the `relu1` activation tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc1_input_count = int(pool1.shape[1] * pool1.shape[2] * pool1.shape[3])\n",
    "fc1_output_count = 1024\n",
    "print([fc1_input_count, fc1_output_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to append a fully connected layer, we need to flatten the spatial dimensions of `relu1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool1_flat = tf.reshape(pool1, [-1, fc1_input_count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to add a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc1 = tf.layers.dense(inputs = pool1_flat, units = 1024, activation = tf.nn.relu, name = 'fc1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add another fully connected layer with bias to output scores for 10 output classes. This layer has no nonlinearity following it, but it will be followed by a softmax function to convert scores to probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_count = 10\n",
    "fc2 = tf.layers.dense(inputs = fc1, units = class_count, name = 'fc2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final classification\n",
    "We append a softmax layer to convert the scores coming from `fc2` into probabilities, as well as a \"top-k\" layer to get the three most probable guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prob = tf.nn.softmax(fc2)\n",
    "(guess_prob, guess_class) = tf.nn.top_k(prob, k = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing parameters and activations\n",
    "TensorBoard supports visualizing tensors as images using `tf.summary.image` function.\n",
    "We add a subnetwork that computes images from `conv1_kernel` and `conv1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('conv1_visualization'):\n",
    "    # Normalize to [0 1].\n",
    "    x_min = tf.reduce_min(conv1_kernel)\n",
    "    x_max = tf.reduce_max(conv1_kernel)\n",
    "    normalized = (conv1_kernel - x_min) / (x_max - x_min)\n",
    "\n",
    "    # Transpose to [batch_size, height, width, channels] layout.\n",
    "    transposed = tf.transpose(normalized, [3, 0, 1, 2])\n",
    "    \n",
    "    # Display random 5 filters from the 32 in conv1.\n",
    "    conv1_kernel_image = tf.summary.image('conv1/kernel', transposed, max_outputs = 3)\n",
    "    \n",
    "    # Do the same for output of conv1.\n",
    "    sliced = tf.slice(conv1, [0, 0, 0, 0], [1, -1, -1, -1])\n",
    "    x_min = tf.reduce_min(sliced)\n",
    "    x_max = tf.reduce_max(sliced)\n",
    "    normalized = (sliced - x_min) / (x_max - x_min)\n",
    "    transposed = tf.transpose(normalized, [3, 1, 2, 0])\n",
    "    conv1_image = tf.summary.image('conv1', transposed, max_outputs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update graph visualization\n",
    "We have added some new nodes, and we need to check if the new graph is OK.\n",
    "To update TensorBoard visualization, we just add a new graph to the event file.\n",
    "The visualizer will pick up the latest graph when its browser tab is refreshed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "Next we run one CIFAR-10 frame through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_random_image():\n",
    "    index = np.random.randint(0, X_train.shape[0])\n",
    "    image = X_train[[index], :, :, :]\n",
    "    label = y_train[[index]]\n",
    "    return index, image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_index, random_image, random_label = choose_random_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A TensorFlow graph is executed by creating a `tf.Session` object and calling its `run` method.\n",
    "A session object encapsulates the control and state of the TensorFlow runtime.\n",
    "The `run` method requires a list of output tensors that should be computed, and a mapping of input tensors to actual data that should be used. For more information, see the TensorFlow [Getting started](https://www.tensorflow.org/get_started/get_started) guide.\n",
    "\n",
    "Optionally we can also specify a device context such as `/cpu:0` or `/gpu:0`. For documentation on this see [this TensorFlow guide](https://www.tensorflow.org/tutorials/using_gpu). The default device is a GPU if available, and a CPU otherwise, so we can skip the device specification from now on.\n",
    "\n",
    "Note: if GPU is explicitly specified, but not available, a Python exception is thrown; current graph is invalidated, and needs to be cleared and rebuilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\") as dev: #\"/cpu:0\" or \"/gpu:0\"\n",
    "        # Initialize weights.\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Map inputs to data.\n",
    "        feed_dict = { X : random_image, y : random_label }\n",
    "\n",
    "        # Set up variables we want to compute.\n",
    "        variables = [guess_prob, guess_class, conv1_kernel_image, conv1_image]\n",
    "\n",
    "        # Perform forward pass.\n",
    "        guess_prob_value, guess_class_value, conv1_kernel_value, conv1_value = sess.run(variables, feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's see the image that was chosen, and networks predictions for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_classification(image, guess_class, guess_prob):\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    for i in range(3):\n",
    "        ind = guess_class[0, i]\n",
    "        prob = guess_prob[0, i]\n",
    "        print(\"Class: {0}\\tProbability: {1:0.0f}%\".format(class_names[ind], prob * 100))\n",
    "    print(\"Ground truth: {0}\".format(class_names[random_label[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classification(random_image[0, :, :, :] + mean_image, guess_class_value, guess_prob_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write generated images to file. After running the next cell the images should be visible in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_summary(conv1_kernel_value)\n",
    "writer.add_summary(conv1_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and metric(s)\n",
    "\n",
    "We append more nodes to compute loss value, and the number of correctly predicted pixels.\n",
    "For loss we use `tf.softmax_cross_entropy_with_logits`. For other loss functions available out of the box in TensorFlow, see https://www.tensorflow.org/api_guides/python/nn#Losses. Of course, you can always build your own custom loss functions from simpler operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup metrics (e.g. loss and accuracy).\n",
    "def setup_metrics(y, y_out):\n",
    "    # Define loss function.\n",
    "    total_loss = tf.nn.softmax_cross_entropy_with_logits(labels = tf.one_hot(y, 10), logits = y_out)\n",
    "    mean_loss = tf.reduce_mean(total_loss)\n",
    "    \n",
    "    # Add top three predictions.\n",
    "    prob = tf.nn.softmax(y_out)\n",
    "    (guess_prob, guess_class) = tf.nn.top_k(prob, k = 3)\n",
    "    \n",
    "    # Compute number of correct predictions.\n",
    "    is_correct = tf.equal(tf.argmax(y_out, 1), y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "    \n",
    "    return mean_loss, accuracy, guess_prob, guess_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be reusing this function later for other architectures.\n",
    "Now we create metrics for our current network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_loss, accuracy, guess_prob, guess_class = setup_metrics(y, fc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing loss and metric(s)\n",
    "We would like to use TensorBoard to visualize loss value and correct count.\n",
    "We add special nodes that generate those logs.\n",
    "\n",
    "We also add a special node that collects all summary nodes in the network. Evaluating this node in a call to `tf.Session.run` causes all summaries to be computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_scalar_summaries():\n",
    "    tf.summary.scalar('mean_loss', mean_loss)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    all_summaries = tf.summary.merge_all()\n",
    "    return all_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_summaries = setup_scalar_summaries()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "Finally, we define the optimization algorithm to be used for training. We use the Adam optimizer with learning rate 5e-4. For other choices see https://www.tensorflow.org/api_guides/python/train#Optimizers.\n",
    "\n",
    "Optimizer's `minimize` method essentially generates a network that performs backward pass based on the forward pass network that we defined, and passed to the optimizer via argument to `minimize`.\n",
    "The result of this method is a dummy node `train_step` which, when evaluated triggers execution of backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def setup_optimizer(loss, learning_rate):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # Batch normalization in TensorFlow requires this extra dependency\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(extra_update_ops):\n",
    "        train_step = optimizer.minimize(loss)\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be reusing this function for other architectures. Now we create optimizer for our current network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = setup_optimizer(mean_loss, 5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an optional backward pass\n",
    "Above we saw how to execute forward pass using `tf.Session.run`. Now we wrap that into a function (since we will be calling it in a loop to train the network). We also add an option to execute a backward pass by passing the extra argument `training`. That way we can use the same function for both training (forward + backward), and evaluation (forward only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_iteration(session, X_data, y_data, training = None):\n",
    "    # Set up variables we want to compute.\n",
    "    variables = [mean_loss, accuracy, guess_prob, guess_class, all_summaries]\n",
    "    if training != None:\n",
    "        variables += [training]\n",
    "        \n",
    "    # Map inputs to data.\n",
    "    feed_dict = { X: X_data, y: y_data, is_training: training != None }\n",
    "            \n",
    "    # Compute variable values, and perform training step if required.\n",
    "    values = session.run(variables, feed_dict = feed_dict)\n",
    "\n",
    "    # Return loss value and number of correct predictions.\n",
    "    return values[:-1] if training != None else values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training/evaluation loop\n",
    "The following is a simple function which trains or evaluates current model for a given number of epochs by repeatedly calling the `run_iteration` function defined above. It also takes care of:\n",
    "* aggregating loss and accuracy values over all minibatches\n",
    "* plotting loss value over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(session, predict, loss_val, Xd, yd,\n",
    "              epochs = 1, batch_size = 64, print_every = 100,\n",
    "              training = None):\n",
    "    \n",
    "    dataset_size = Xd.shape[0]\n",
    "    batches_in_epoch = int(math.ceil(dataset_size / batch_size))\n",
    "\n",
    "    # Shuffle indices.\n",
    "    train_indices = np.arange(dataset_size)\n",
    "    np.random.shuffle(train_indices)\n",
    "\n",
    "    # Count iterations since the beginning of training. \n",
    "    iter_cnt = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # Keep track of performance stats (loss and accuracy) in current epoch.\n",
    "        total_correct = 0\n",
    "        losses = []\n",
    "        \n",
    "        # Iterate over the dataset once.\n",
    "        for i in range(batches_in_epoch):\n",
    "\n",
    "            # Indices for current batch.\n",
    "            start_idx = (i * batch_size) % dataset_size\n",
    "            idx = train_indices[start_idx : (start_idx + batch_size)]\n",
    "            \n",
    "            # Get batch size (may not be equal to batch_size near the end of dataset).\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "            \n",
    "            loss, acc, _, _, summ = run_iteration(session, Xd[idx,:], yd[idx], training)\n",
    "\n",
    "            # Update performance stats.\n",
    "            losses.append(loss * actual_batch_size)\n",
    "            total_correct += acc * actual_batch_size\n",
    "            \n",
    "            # Add summaries to event file.\n",
    "            if (training is not None):\n",
    "                writer.add_summary(summ, e * batches_in_epoch + i)\n",
    "            \n",
    "            # Print status.\n",
    "            if (training is not None) and (iter_cnt % print_every) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2f}%\"\\\n",
    "                      .format(iter_cnt, loss, acc * 100))\n",
    "            iter_cnt += 1\n",
    "\n",
    "        # Compute performance stats for current epoch.\n",
    "        total_accuracy = total_correct / dataset_size\n",
    "        total_loss = np.sum(losses) / dataset_size\n",
    "\n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.2f}%\"\\\n",
    "              .format(total_loss, total_accuracy * 100, e + 1))\n",
    "    return total_loss, total_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model for one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "print('Training')\n",
    "run_model(sess, fc2, mean_loss, X_train, y_train, 1, 64, 100, train_step)\n",
    "print('Validation')\n",
    "_ = run_model(sess, fc2, mean_loss, X_val, y_val, 1, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View summaries in TensorBoard log\n",
    "Now you should be able to refresh your TensorBoard tab and see the summaries.\n",
    "For more details please see [official tutorial](https://www.tensorflow.org/get_started/summaries_and_tensorboard) on summaries.\n",
    "TensorFlow also supports other kinds of summaries, such as [histograms](https://www.tensorflow.org/get_started/tensorboard_histograms). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize some predictions\n",
    "Accuracy should be somewhat better now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_index, random_image, random_label = choose_random_image()\n",
    "_, _, guses_prob_value, guess_class_value, _ = run_iteration(sess, random_image, random_label)\n",
    "visualize_classification(random_image[0, :, :, :] + mean_image, guess_class_value, guess_prob_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add batch normalization\n",
    "\n",
    "We modify the simple model by adding batch normalization after the convolution layer. We expect this network to train faster, and achieve better accuracy for the same number of weight updates.\n",
    "\n",
    "For convenience, we collect previous network definition code into a function `bn_model`. One difference from the above code is the line that defines the batch normalization layer. Parameters of `bn_model` are input placeholders, and its return value is the output tensor (plus any other tensors that are needed outside the function, i.e. for inspection or visualization). Another input parameter `is_training` is used to select between training and inference version of the network, because batch normalization behaves differently during training and inference.\n",
    "\n",
    "All parameter and activation shapes are the same as before, since batch normalization does not modify the shape of its input. Hence `fc1_input_count` and `fc1_output_count` computed above are valid.\n",
    "\n",
    "API reference for batch normalization is at https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bn_model(X, y, is_training):\n",
    "    # Convolution layer.\n",
    "    conv1 = tf.layers.conv2d(inputs = X, filters = 32, kernel_size = [7, 7], strides = 2, padding = 'SAME', activation=tf.nn.relu, name = 'conv1')\n",
    "    \n",
    "    # Batch normalization layer.\n",
    "    bn1 = tf.layers.batch_normalization(conv1, training = is_training)\n",
    "\n",
    "    # Pooling layer.\n",
    "    pool1 = tf.layers.max_pooling2d(inputs = bn1, pool_size = [2, 2], strides = 2, padding = 'SAME', name = 'pool1')\n",
    "\n",
    "    # First fully connected layer.\n",
    "    pool1_flat = tf.reshape(pool1,[-1, fc1_input_count])\n",
    "    fc1 = tf.layers.dense(inputs = pool1_flat, units = 1024, activation = tf.nn.relu, name = 'fc1')\n",
    "    \n",
    "    # Second fully connected layer.\n",
    "    fc2 = tf.layers.dense(inputs = fc1, units = class_count, name = 'fc2')\n",
    "\n",
    "    return fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input, metrics and optimizer are the same as before, so we can assemble the whole network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "writer = tf.summary.FileWriter(log_dir)\n",
    "X, y, is_training = setup_input()\n",
    "y_out = bn_model(X, y, is_training)\n",
    "mean_loss, accuracy, guess_prob, guess_class = setup_metrics(y, y_out)\n",
    "all_summaries = setup_scalar_summaries()\n",
    "train_step = setup_optimizer(mean_loss, 5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train and validate the network with batch normalization as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    run_model(sess, y_out, mean_loss, X_train, y_train, 1, 64, 100, train_step)\n",
    "    print('Validation')\n",
    "    run_model(sess, y_out, mean_loss, X_val, y_val, 1, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: build given architecture\n",
    "\n",
    "Your task is now to build the architecture contained in event file `cifar10_net_log\\cifar10_net_log`, and train it on CIFAR-10. You should train for 8 epochs with batch size 100 and learning rate 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar10_net(X, y, is_training):\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "writer = tf.summary.FileWriter(log_dir)\n",
    "X, y, is_training = setup_input()\n",
    "y_out = cifar10_net(X, y, is_training)\n",
    "mean_loss, accuracy, guess_prob, guess_class = setup_metrics(y, y_out)\n",
    "all_summaries = setup_scalar_summaries()\n",
    "train_step = setup_optimizer(mean_loss, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print('Training')\n",
    "    run_model(sess, y_out, mean_loss, X_train, y_train, 8, 100, 100, train_step)\n",
    "    print('Validation')\n",
    "    run_model(sess, y_out, mean_loss, X_val, y_val, 1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading models\n",
    "\n",
    "Saving is done using `tf.train.Saver` class:\n",
    "* `save` method saves both network definition and weights. \n",
    "* `export_meta_graph` method saves only network definition.\n",
    "\n",
    "Loading is done in two stages:\n",
    "* `tf.train.import_meta_graph` function loads network definition, and returns a saver object that was used to save the model.\n",
    "* `restore` method of the returned saver object loads the weights.\n",
    "\n",
    "Note that since weights are available only inside a session, `save` and `restore` methods above require a session object as a parameter.\n",
    "\n",
    "Official TensorFlow documentation: [Saving and Restoring Variables](https://www.tensorflow.org/api_guides/python/state_ops#Saving_and_Restoring_Variables), [tf.train.Saver class](https://www.tensorflow.org/api_docs/python/tf/train/Saver), [tf.train.import_meta_graph function](https://www.tensorflow.org/api_docs/python/tf/train/import_meta_graph).\n",
    "\n",
    "Useful unofficial tutorial on saving and loading: http://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning\n",
    "\n",
    "In this section we will start from a model which is pretrained on ImageNet, and finetune it for our CIFAR-10 recognition task.\n",
    "\n",
    "Pretrained model is given by meta-graph file (containing network definition), and checkpoint file (containing weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_meta_graph = r\"inception_resnet_v2\\inception_resnet_v2_2016_08_30.meta\"\n",
    "pretrained_checkpoint = r\"inception_resnet_v2\\inception_resnet_v2_2016_08_30.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the CIFAR-10 classification task we need to perform these two modifications to the pretrained mode at the very minimum:\n",
    "* Process CIFAR-10 images so that their size becomes what pretrained model expects\n",
    "* Adapt the last fully connected layer (which does final classification) so that the number of output neurons is 10 (the number of classes in the CIFAR-10 classification task)\n",
    "\n",
    "### Get names of relevant nodes\n",
    "\n",
    "Modifying input part of a pretrained network is somewhat cumbersome. It has to be done simultaneously with loading network definition, by passing to `tf.train.import_meta_graph` a mappping from input tensors of the pretrained network to new input tensors.\n",
    "\n",
    "First we load pretrained network definition only to get the names of input placeholder nodes that we want to replace. This step can be skipped if these names are already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "writer = tf.summary.FileWriter(log_dir)\n",
    "_ = tf.train.import_meta_graph(pretrained_meta_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the nodes' names using TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can do it programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    if op.type == 'Placeholder':\n",
    "        print(op.outputs[0].name + '\\t' + str(op.outputs[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the name of the fully connected layer that does final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    if op.type == \"MatMul\":\n",
    "        print('Operation: ' + op.name)\n",
    "        print('Inputs: ')\n",
    "        for inp in op.inputs:\n",
    "            print(inp.name + '\\t' + str(inp.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify input and output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we clear the default graph, and start creating new one, with modified input subnetwork which, which upsamples input image to match the size expected by pretrained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "writer = tf.summary.FileWriter(log_dir)\n",
    "X, y, is_training = setup_input()\n",
    "X_upsampled = tf.image.resize_images(X, [299, 299])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we reload pretrained network definition, replacing pretrained input placeholders with new tensors we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(pretrained_meta_graph,\n",
    "    input_map = { 'Placeholder:0' : X_upsampled, 'Placeholder_2:0' : is_training })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a handle to the tensor containing classification features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat = tf.get_default_graph().get_tensor_by_name(\"InceptionResnetV2/Logits/Dropout/cond/Merge:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attach a new fully connected layer for modified task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_count = 10\n",
    "fc1 = tf.layers.dense(feat, class_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete network definition\n",
    "Add metrics and optimizer as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_loss, accuracy, guess_prob, guess_class = setup_metrics(y, fc1)\n",
    "all_summaries = setup_scalar_summaries()\n",
    "train_step = setup_optimizer(mean_loss, 5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again write out the graph to make sure surgery succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and validate network (at your own risk!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only now we can restore weights from checkpoint, because weights exist only inside a session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, pretrained_checkpoint)\n",
    "    print('Training')\n",
    "    run_model(sess, y_out, mean_loss, X_train, y_train, 1, 64, 100, train_step)\n",
    "    print('Validation')\n",
    "    run_model(sess, y_out, mean_loss, X_val, y_val, 1, 64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
